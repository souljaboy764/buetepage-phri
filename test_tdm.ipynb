{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Human VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import *\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import networks\n",
    "from utils import *\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Testing Trajecotries\n"
     ]
    }
   ],
   "source": [
    "tdm_ckpt = 'logs/2023/04202126/tdm/models/tdm_final.pth' # input()\n",
    "\n",
    "tdm_hyperparams = np.load(os.path.join(os.path.dirname(tdm_ckpt),'tdm_hyperparams.npz'), allow_pickle=True)\n",
    "tdm_args = tdm_hyperparams['args'].item()\n",
    "tdm = networks.TDM(**(tdm_hyperparams['tdm_config'].item().__dict__)).to(device)\n",
    "tdm.load_state_dict(torch.load(tdm_ckpt)['model'])\n",
    "\n",
    "vae_hyperparams = np.load(os.path.join(os.path.dirname(tdm_args.vae_ckpt),'hyperparams.npz'), allow_pickle=True)\n",
    "vae_args = vae_hyperparams['args'].item()\n",
    "vae = getattr(networks, vae_args.model)(**(vae_hyperparams['vae_config'].item().__dict__)).to(device)\n",
    "\n",
    "vae.load_state_dict(torch.load(tdm_args.vae_ckpt)['model'])\n",
    "vae.eval()\n",
    "\n",
    "with np.load(tdm_args.src, allow_pickle=True) as data:\n",
    "\ttest_data_np = data['test_data']\n",
    "\ttest_data = [torch.Tensor(traj) for traj in test_data_np]\n",
    "\ttest_num = len(test_data)\n",
    "\tprint(test_num,'Testing Trajecotries')\n",
    "\tlens = []\n",
    "\tfor traj in test_data:\n",
    "\t\tlens.append(traj.shape[0])\n",
    "\tpadded_sequences = pad_sequence(test_data, batch_first=True, padding_value=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 torch.Size([39, 1261, 965])\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len, dims = padded_sequences.shape\n",
    "mask = torch.arange(seq_len).unsqueeze(0).repeat(batch_size,1) < lens.unsqueeze(1).repeat(1,seq_len)\n",
    "x1_tdm = padded_sequences[:,:,p1_tdm_idx].to(device)\n",
    "# x2_tdm = x[:,:,p2_tdm_idx].to(device)\n",
    "x1_vae = padded_sequences[:,:,p1_vae_idx].to(device)\n",
    "x2_vae = padded_sequences[:,:,p2_vae_idx].to(device)\n",
    "\n",
    "x1_vae_out, _, _ = vae(x1_vae)\n",
    "x2_vae_out, _, _ = vae(x2_vae)\n",
    "\n",
    "# didn't fully understand how p(d|h_1) can be used to generate p(z_1|d) and p(z_2|d) \n",
    "# since the generated z_1 and z_2 (and x_1 and x_2) would belong to the same general distribution obtained from p(d|h_1)\n",
    "#\n",
    "# Is it the case that they have two separate AEs for each actor even in the HH case?\n",
    "z_d1_dist, d1_samples, d1_dist = tdm(torch.nn.utils.rnn.pack_padded_sequence(x1_tdm, lens, batch_first=True, enforce_sorted=False), seq_len)\n",
    "x_tdm_out = vae._output(vae._decoder(z_d1_dist.mean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import asyncio\n",
    "\n",
    "fig, ax = prepare_axis()\n",
    "async def update():\n",
    "    global ax\n",
    "    for frame_idx in range(1000):\n",
    "        ax = reset_axis(ax)\n",
    "        ax = visualize_skeleton(ax, test_data[frame_idx], markerfacecolor='r', linestyle='-', alpha=0.5)\n",
    "        ax = visualize_skeleton(ax, x_gen[frame_idx], markerfacecolor='m', linestyle='--', alpha=0.2)\n",
    "\n",
    "        test_data[N+frame_idx, ..., 0] = 0.7 - test_data[N+frame_idx, ..., 0]\n",
    "        test_data[N+frame_idx, ..., 1] = 0.2 - test_data[N+frame_idx, ..., 1]\n",
    "        x_gen[N+frame_idx, ..., 0] = 0.7 - x_gen[N+frame_idx, ..., 0]\n",
    "        x_gen[N+frame_idx, ..., 1] = 0.2 - x_gen[N+frame_idx, ..., 1]\n",
    "\n",
    "        ax = visualize_skeleton(ax, test_data[N+frame_idx], markerfacecolor='b', linestyle='-', alpha=0.5)\n",
    "        ax = visualize_skeleton(ax, x_gen[N+frame_idx], markerfacecolor='g', linestyle='--', alpha=0.2)\n",
    "        \n",
    "        fig.canvas.draw_idle()\n",
    "        fig.canvas.flush_events()\n",
    "        await asyncio.sleep(0.001)\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.create_task(update());\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
